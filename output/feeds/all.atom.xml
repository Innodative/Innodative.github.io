<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>The Innodative Disruptor</title><link href="https://innodative.com/" rel="alternate"/><link href="https://innodative.com/feeds/all.atom.xml" rel="self"/><id>https://innodative.com/</id><updated>2025-09-17T00:00:00-05:00</updated><entry><title>The Quantum You Want</title><link href="https://innodative.com/posts/quantum-you-want/" rel="alternate"/><published>2025-09-17T00:00:00-05:00</published><updated>2025-09-17T00:00:00-05:00</updated><author><name>Your Name</name></author><id>tag:innodative.com,2025-09-17:/posts/quantum-you-want/</id><summary type="html">&lt;p&gt;A clear, accessible explanation of why quantum computing represents a fundamentally different approach to computation and where its most significant early impacts are likely to occur.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;span class="newthought"&gt;Throughout our history, humanity has striven to predict the future.&lt;/span&gt; From astrology and crystal balls to modern science, we look for an edge in what is to come. Consider the weather, something we often discuss even with strangers. Your smartphone likely has one or more apps that will tell you whether it will be cloudy or sunny today, or if storms are coming. While we often focus on the veracity—or lack thereof—of these predictions, perhaps a more important observation is the power of the device itself.&lt;/p&gt;
&lt;p&gt;We rarely dwell on it, but your smartphone would rival the top supercomputers in the world from the turn of the century. And while few predicted the incredible power we now carry in a pocket or purse, both smartphones and supercomputers function in a similar manner. Data from the world around us—whether temperature measurements, texts, music playlists, or YouTube videos—are converted into long streams of zeros and ones, then processed by computer chips that follow deterministic rules.&lt;/p&gt;
&lt;p&gt;This highlights an important point. Despite the historical obsession, humanity is still bad at making predictions. A good summary of this state of affairs comes from Bill Gates, who in 1996 wrote:&lt;label for="sn-gates" class="margin-toggle sidenote-number"&gt;&lt;/label&gt;&lt;input type="checkbox" id="sn-gates" class="margin-toggle"/&gt;&lt;span class="sidenote"&gt;Bill Gates, &lt;em&gt;The Road Ahead&lt;/em&gt;, Afterword, p. 316, Penguin Books, 1996&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;People often overestimate what will happen in the next two years and underestimate what will happen in ten. I'm guilty of this myself.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of course, this prediction paradox applies perfectly to computing. Today's top supercomputers are roughly a million times more powerful than your smartphone. Yet even these behemoths are often unable to predict the weather a few days out. As Mick Jagger eloquently sang, "You can't always get what you want."&lt;label for="sn-stones" class="margin-toggle sidenote-number"&gt;&lt;/label&gt;&lt;input type="checkbox" id="sn-stones" class="margin-toggle"/&gt;&lt;span class="sidenote"&gt;But as the song continues, &lt;a href="https://music.apple.com/us/song/you-cant-always-get-what-you-want-remastered-2019/1500643065"&gt;you get what you need&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;h2 id="the-computing-you-want"&gt;The Computing You Want&lt;/h2&gt;
&lt;p&gt;While some might argue for better algorithms or new hardware designs, we may simply need an entirely different approach—one first postulated by Nobel prize-winning physicist Richard Feynman. At a conference on computers in physics in 1981, he introduced the concept of using quantum mechanical systems for computation, famously stating:&lt;label for="sn-feynman" class="margin-toggle sidenote-number"&gt;&lt;/label&gt;&lt;input type="checkbox" id="sn-feynman" class="margin-toggle"/&gt;&lt;span class="sidenote"&gt;For more information, you can &lt;a href="https://link.springer.com/article/10.1007/BF02650179"&gt;read the paper&lt;/a&gt; he submitted following the conference.&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Nature isn't classical, dammit, and if you want to make a simulation of nature, you'd better make it quantum mechanical.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now Feynman was generally referring to simulations of molecules and materials that involve fundamental physics or chemistry, rather than the weather. But this charge helped spur others to explore this new idea. And now, over four decades later, we are at the cusp of a new revolution in computing, one based on the principles of quantum mechanics. For many, any exposure to the quantum world and the probabilistic nature it embodies can end in confusion. For example, in the classical world, you can know both where a car is located and how fast it is traveling. But in the quantum realm, knowing one of these, either position or velocity, means we can't precisely know the other. This isn't due to any fault in how we measure them, but is instead a fundamental property of nature described by the Heisenberg Uncertainty Principle.&lt;label for="sn-heisenberg" class="margin-toggle sidenote-number"&gt;&lt;/label&gt;&lt;input type="checkbox" id="sn-heisenberg" class="margin-toggle"/&gt;&lt;span class="sidenote"&gt;&lt;a href="https://en.wikipedia.org/wiki/Uncertainty_principle"&gt;A remarkably simple yet profound result&lt;/a&gt;, first shown in 1927 by the Nobel prize-winning physicist Werner Heisenberg. More formally, there is an inherent limitation in how well we can know the product of the position and momentum of a particle. This uncertainty becomes important when dealing with fundamental particles of nature like the electron or proton.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus, it is unsurprising that quantum computers are not as easy to build, use, or understand as your smartphone. Classical computers work with bits that are either zero or one—on or off. Quantum computers are based on quantum bits, or qubits, which, being quantum mechanical, have some surprising behaviors. The two most relevant for our discussion are superposition and entanglement.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Superposition&lt;/strong&gt; means that a qubit can be in multiple states at once, like a coin that is simultaneously both heads and tails, until you observe it. In contrast, a classical bit is like a coin that has already landed: it's either heads or tails, but never both. While this phenomenon may seem abstract, combining multiple qubits causes the power of superposition to grow exponentially: two qubits can exist in four possible states, ten qubits in over a thousand, and twenty qubits in over a million. Through superposition, quantum computers can encode and manipulate many possible states simultaneously within the same system.&lt;/p&gt;
&lt;p&gt;On its own, this provides unique avenues to reinvent computation. However, that's only part of the story. The second quantum property—&lt;strong&gt;entanglement&lt;/strong&gt;—means that qubits can become deeply linked, such that knowing something about one instantly reveals information about the other, even if they're far apart.&lt;label for="sn-einstein" class="margin-toggle sidenote-number"&gt;&lt;/label&gt;&lt;input type="checkbox" id="sn-einstein" class="margin-toggle"/&gt;&lt;span class="sidenote"&gt;If this seems troubling, you are in good company. &lt;a href="https://www.technologyreview.com/2012/03/08/20152/einsteins-spooky-action-at-a-distance-paradox-older-than-thought/"&gt;Einstein referred to this process&lt;/a&gt; as spooky action at a distance.&lt;/span&gt; Imagine we each flip a pair of entangled coins. If I look at mine, we instantly know what yours shows without having to look. This idea, combined with sufficiently clever algorithms, allows many entangled qubits to be processed simultaneously. As a result, we can navigate what was once an intractably complex landscape to reach the desired endpoint with incredible speed.&lt;/p&gt;
&lt;h2 id="the-quantum-you-find"&gt;The Quantum You Find&lt;/h2&gt;
&lt;p&gt;This promise has driven large companies like IBM, Microsoft, and Google, as well as smaller players such as D-Wave, IonQ, and PsiQuantum, to explore different qubit architectures and to construct and deploy multi-qubit systems. Yet the physical challenges of building and operating quantum systems, especially at the scales needed for real-world problems, remain an unsolved challenge. Simply put, quantum systems lack the commodity approach of classical systems that rely on semiconductors—each company building in this space seems to have a different approach.&lt;/p&gt;
&lt;p&gt;Despite these challenges, the development of quantum algorithms has not stopped, allowing us to predict where quantum computers will have the biggest initial impact. And, spoiler alert, it won't be running Excel or watching YouTube. Instead, quantum systems should excel at these four tasks:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finding needles in a haystack:&lt;/strong&gt; Quantum computers can quickly search through large data sets to find targets, which can be useful for database searching and pattern matching.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solving optimization problems:&lt;/strong&gt; Quantum computers can identify the optimal solution out of a very large number of possibilities, which is useful in supply chain logistics and financial modeling.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cryptography:&lt;/strong&gt; Quantum computers can quickly break traditional encryption standards, which threatens global communication and financial transactions. Fortunately, quantum computing also offers new approaches that are immune to these threats.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Simulating Nature:&lt;/strong&gt; At its core, nature follows the rules of quantum mechanics, thus quantum computing, as envisioned by Feynman, can naturally simulate molecules, chemical reactions, and material properties. This could lead to the development of personalized medicines and tailored materials that outperform existing options.&lt;/p&gt;
&lt;h2 id="when-will-quantum-arrive"&gt;When Will Quantum Arrive?&lt;/h2&gt;
&lt;p&gt;So the million-dollar question is when will a quantum computer be available? Many industry experts predict the first working, general-purpose quantum computer—known as Q-Day—will arrive within five or ten years.&lt;label for="sn-qday" class="margin-toggle sidenote-number"&gt;&lt;/label&gt;&lt;input type="checkbox" id="sn-qday" class="margin-toggle"/&gt;&lt;span class="sidenote"&gt;After raising the difficulty in making predictions I hesitate to do so here. So I simply leverage AI to &lt;a href="https://www.ibm.com/quantum/blog/large-scale-ftqc"&gt;company&lt;/a&gt;, &lt;a href="https://globalriskinstitute.org/publication/briefing-note-recent-updates-on-quantum-timeline/"&gt;expert&lt;/a&gt;, and &lt;a href="https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-year-of-quantum-from-concept-to-reality-in-2025"&gt;consultancy&lt;/a&gt; predictions.&lt;/span&gt; While that may seem optimistic, it is also prudent to recall Gates's quote. Maybe you will soon know unambiguously whether to pack that umbrella or not!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Originally published by the &lt;a href="https://giesbusiness.illinois.edu/news/2025/10/16/the-quantum-you-want--qubits--forecasts--and-the-next-tech-revolution"&gt;Gies College of Business&lt;/a&gt; on October 16, 2025.&lt;/p&gt;</content><category term="Thoughts"/></entry><entry><title>Dancing in the Dark</title><link href="https://innodative.com/posts/dancing-in-the-dark/" rel="alternate"/><published>2025-07-22T00:00:00-05:00</published><updated>2025-07-22T00:00:00-05:00</updated><author><name>Your Name</name></author><id>tag:innodative.com,2025-07-22:/posts/dancing-in-the-dark/</id><summary type="html">&lt;p&gt;A reflection on how generative AI has disrupted academia, leaving faculty uncertain, uneasy, and searching for guidance as they adapt to a rapidly changing landscape.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;span class="newthought"&gt;Popular media regularly characterizes academics as occupants of an ivory tower,&lt;/span&gt; perhaps with hints of derision as the occupants are presumably disconnected from practical concerns. Faculty have historically been isolated from real world disruptions caused by technology, such as the computer or the Internet, which diffused from academic labs into industry. The Ivory Tower provided refuge.&lt;/p&gt;
&lt;p&gt;But this metaphorical barrier has been frequently pierced of late. The primary drivers behind emerging technology disruptions are no longer academic or government labs but corporations and private firms. And nowhere is this more evident than with Generative AI, which has been developed and disseminated primarily by researchers working for deep pocketed technology firms.&lt;label for="sn-1" class="margin-toggle sidenote-number"&gt;&lt;/label&gt;&lt;input type="checkbox" id="sn-1" class="margin-toggle"/&gt;&lt;span class="sidenote"&gt;Competition between technology firms (and countries) has created an &lt;a href="https://www.cnbc.com/2025/06/18/sam-altman-says-meta-tried-to-poach-openai-staff-with-100-million-bonuses-mark-zuckerberg.html"&gt;arms race for talent&lt;/a&gt; and resources.&lt;/span&gt; This origination story impacts faculty in two important ways. First, Generative AI tools, like ChatGPT, developed rapidly and were released to an unprepared public, leaving many feeling flat-footed by this seemingly magical technology. Second, and perhaps most importantly, this technology primarily impacts what is known as knowledge work. Simply put, Generative AI cuts to the heart of what it means to be an academic; The Ivory Tower no longer provides refuge.&lt;/p&gt;
&lt;p&gt;We are supposed to be experts; yet, in this case, we are behind the curve, unsure of how to react or even what to do. It is as if a universal case of imposter syndrome has descended upon the Ivory Tower as we struggle to understand how to embrace, explain, or use this technology. We are, in effect, &lt;em&gt;dancing in the dark&lt;/em&gt;:&lt;label for="sn-2" class="margin-toggle sidenote-number"&gt;&lt;/label&gt;&lt;input type="checkbox" id="sn-2" class="margin-toggle"/&gt;&lt;span class="sidenote"&gt;This phrase was popularized in a &lt;a href="https://brucespringsteen.net/track/dancing-in-the-dark/"&gt;hit song by Bruce Springsteen&lt;/a&gt; that dealt with the themes of isolation and frustration with work, which seems rather relevant to this discussion.&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;unsure if we are alone or in a crowd&lt;/li&gt;
&lt;li&gt;unsure if we are doing the right things&lt;/li&gt;
&lt;li&gt;unsure if we are improving or regressing&lt;/li&gt;
&lt;li&gt;unsure if this is the beginning of the end or the end of the beginning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="the-disruption-of-academia"&gt;The Disruption of Academia&lt;/h2&gt;
&lt;p&gt;The first interaction for many of us was not a good one—a sudden wave of Generative AI-generated answers to writing assignments (ironically characterized by an excessive use of the em dash) that threatened a crucial assessment technique, and more broadly the concept of academic integrity. The resultant outcry was swift and perhaps predictable. Many called for bans, some called for patience and reflection, and some the Persian adage: "This too shall pass."&lt;/p&gt;
&lt;p&gt;But in the interim, we have learned several important lessons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generate AI is not going away.&lt;/li&gt;
&lt;li&gt;Generative AI is continually improving.&lt;/li&gt;
&lt;li&gt;Generative AI proficiency is a required skill for our graduates.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus, if we are forward thinking we must embrace the adoption of AI, or risk growing increasingly irrelevant within our ivory towers. Yet, we have little clarity or guidance on how to best move into this brave, new world. Either from our administrations or peers—we are still dancing in the dark! And, as knowledge workers, we are further confronted by the discomforting hint of the ephemeral nature of our our own careers.&lt;/p&gt;
&lt;p&gt;We spend years mastering a subject, becoming the expert in the room. But now, Generative AI can summarize an entire research field, customize our course lecture notes for each student, or critique a case study with unnerving fluency. The challenge is less about AI being smarter and more that AI is faster, convincing, and tireless. The comparison, and competition, is unsettling. This leads to a new imposter syndrome where faculty ask, "Am I falling behind?" and "Is my expertise still relevant?".&lt;/p&gt;
&lt;p&gt;Alongside these doubts are real ethical concerns. Faculty may be asking:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What counts as "my work" if I use AI?&lt;/li&gt;
&lt;li&gt;Am I allowed to use AI to provide student feedback?&lt;/li&gt;
&lt;li&gt;Am I violating academic integrity standards if I use AI too much or too little?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Without clear institutional guidance and support, many faculty are adrift in an ethical fog, unsure how or even if they should proceed. And these feelings apply equally across our service, teaching, and research&lt;label for="sn-3" class="margin-toggle sidenote-number"&gt;&lt;/label&gt;&lt;input type="checkbox" id="sn-3" class="margin-toggle"/&gt;&lt;span class="sidenote"&gt;Experimentation is already happening on AI augmented research, see the &lt;a href="https://agents4science.stanford.edu/index.html"&gt;Agents4Science&lt;/a&gt; conference.&lt;/span&gt; roles.&lt;/p&gt;
&lt;h2 id="a-guiding-metaphor"&gt;A Guiding Metaphor&lt;/h2&gt;
&lt;p&gt;Fortunately, there is a simple step we can take. Embrace the future and learn to work effectively with AI. Rather than seeing AI as a threat, view AI as a skilled intern that can quickly draft, summarize, analyze, and even organize large corpora. But this, in turn, requires us to provide direction, oversight, and judgment. Of course, this is the same advice we should be giving to our students, who are increasingly expected to master these skills to gain employment.&lt;/p&gt;
&lt;p&gt;While easy to say, doing this can be discomforting. But this is not a new concept. In 1987, Apple released a concept video entitled the "Knowledge Navigator" that showed a professor interacting with a highly competent digital assistant.&lt;label for="mn-1" class="margin-toggle"&gt;⊕&lt;/label&gt;&lt;input type="checkbox" id="mn-1" class="margin-toggle"/&gt;&lt;span class="marginnote"&gt;&lt;a href="https://www.youtube.com/watch?v=umJsITGzXd0"&gt;Knowledge Navigator video&lt;/a&gt;&lt;/span&gt; This bow-tied AI helped schedule meetings, find articles of interest, and organize research. While inspirational, it was at the time, science fiction.&lt;/p&gt;
&lt;p&gt;But today this dream is alive. Faculty can ask AI to generate lecture outlines, critique writing, summarize recent research, or simulate a conversation in another language. What was once imagined as a distant future is now relatively cheap and ubiquitous. Yet faculty are still struggling as they ask, "How do I get started?", "What is permitted under university policy?", or "How do I remain a role model for my students?".&lt;/p&gt;
&lt;p&gt;The initial goal shouldn't be to master AI, but to learn to use it meaningfully. Faculty must be the trusted guides into a world where AI influences how knowledge is produced, shared, and valued. For this to happen, institutions must provide the supporting scaffolding for a confident adoption of AI. They must:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Normalize discomfort.&lt;/strong&gt; Faculty must be able to share their concerns and experiences without fear of judgment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Provide clear guidelines.&lt;/strong&gt; Institutions should articulate what is allowed and what is not.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Promote mentorship and peer learning.&lt;/strong&gt; Small groups who share use cases, failures, and lessons.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recognize ethical creativity.&lt;/strong&gt; Highlight examples of AI-enhanced assignments, not just AI detection tools.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encourage transparency.&lt;/strong&gt; Let students know when and how AI is used—and expect the same from them.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Just as the Knowledge Navigator inspired an earlier generation, today's Generative AI tools invite us to rethink how we teach, learn, and lead. We can stop dancing in the dark! With an open mindset, shared wisdom, and institutional support, the lights will come on and our path forward becomes not only visible, but empowering.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Originally published by the &lt;a href="https://giesbusiness.illinois.edu/news/2025/08/27/dancing-in-the-dark--academia-s-reckoning-with-generative-ai"&gt;Gies College of Business&lt;/a&gt; on August 27, 2025.&lt;/p&gt;</content><category term="Thoughts"/></entry><entry><title>Ghosts in the Machine</title><link href="https://innodative.com/posts/ghosts-in-the-machine/" rel="alternate"/><published>2025-07-02T00:00:00-05:00</published><updated>2025-07-02T00:00:00-05:00</updated><author><name>Your Name</name></author><id>tag:innodative.com,2025-07-02:/posts/ghosts-in-the-machine/</id><summary type="html">&lt;p&gt;An exploration of how generative AI embodies the accumulated knowledge of humanity and is reshaping both entry-level work and the mission of higher education.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;span class="newthought"&gt;Humans love to anthropomorphize.&lt;/span&gt; From seeing objects in cloud shapes to naming physical devices, we humanize our environment, making complex things easier to understand and to describe to others. This approach has carried over into the world of artificial intelligence, with the creation of named tools like Alexa and Siri, or when ChatGPT says "Let me know if ..." within its response.&lt;/p&gt;
&lt;p&gt;On the face of it, this may seem odd. But generative AI tools like ChatGPT and Claude aren't just code—they are algorithms defined by the shared knowledge of our species. Thus, if you ask ChatGPT to explain special relativity in the voice of Shakespeare, it may seem as if the ghosts of Einstein and Shakespeare are collaborating to quickly generate the result. While this contrived example may have little bearing on the world of business, the embodied spirits in the model are not limited to these historical examples. In this analogy, these large language models are haunted by the cumulative expertise of billions of humans, or the &lt;em&gt;ghosts in the machine&lt;/em&gt;, whose digital legacies are encoded in their parameters.&lt;/p&gt;
&lt;p&gt;Ironically, the phrase "Ghost in the Machine" was coined by British philosopher Gilbert Ryle as a criticism of the idea that the mind could be considered separate from the body. Now, we use this phrase to describe our interactions with a super-powerful, non-corporeal digital mind—Generative AI. But these interactions are not limited to generating essays; these "ghosts in the machine", trained on numerous examples, are now completing tasks once exclusively reserved for entry-level workers in marketing, finance, HR, and sales.&lt;/p&gt;
&lt;p&gt;This begs the questions: if AI becomes the default "junior hire," what becomes of recent graduates? And how should higher education—especially business schools—adapt to this new reality?&lt;/p&gt;
&lt;h2 id="the-disruption-of-entry-level-work"&gt;The Disruption of Entry Level Work&lt;/h2&gt;
&lt;p&gt;As educators, our first thought around AI tends to be how can we ensure the academic integrity of our classes? While this question has merit, by focusing on the &lt;em&gt;how&lt;/em&gt; we educate, we risk missing the more important questions of &lt;em&gt;why&lt;/em&gt; and &lt;em&gt;what&lt;/em&gt; we educate. As the hyperbole around AI has grown, more students and their parents are questioning the value of a traditional college. These concerns follow from studies such as the well-covered State of the Tech Talent report by SignalGate&lt;label for="mn-signal" class="margin-toggle"&gt;⊕&lt;/label&gt;&lt;input type="checkbox" id="mn-signal" class="margin-toggle"/&gt;&lt;span class="marginnote"&gt;&lt;a href="https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025"&gt;State of the Tech Talent report&lt;/a&gt;&lt;/span&gt; that showed a 50% reduction in new graduate hires by big technology firms from pre-pandemic levels. More recently, Dario Amodej, the CEO of Anthropic, a leading AI firm, stated that up to 50 percent of entry-level white-collar jobs could be eliminated by increased adoption of AI tools (i.e., the ghosts in the machine) over the next five years. Why should we expect that business as usual is still a viable approach for higher education? Will the ghosts in the machine take over?&lt;/p&gt;
&lt;p&gt;To be clear, Amodej's warning carries weight, but maybe the truth is more nuanced. Executives from OpenAI, a rival AI firm, paint a different picture. Brad Lightcap, COO of OpenAI argues that rather than AI eliminating entry-level roles, companies will embrace young people&lt;label for="mn-lightcap" class="margin-toggle"&gt;⊕&lt;/label&gt;&lt;input type="checkbox" id="mn-lightcap" class="margin-toggle"/&gt;&lt;span class="marginnote"&gt;&lt;a href="https://www.youtube.com/watch?v=cT63mvqN54o"&gt;Brad Lightcap interview&lt;/a&gt;&lt;/span&gt; who have a level of fluency with AI that far transcends anyone else at those organizations. Likewise, Sam Altman, CEO of OpenAI emphasizes that AI will make employees more productive,&lt;label for="mn-altman" class="margin-toggle"&gt;⊕&lt;/label&gt;&lt;input type="checkbox" id="mn-altman" class="margin-toggle"/&gt;&lt;span class="marginnote"&gt;&lt;a href="https://blog.samaltman.com/the-gentle-singularity"&gt;Sam Altman, 'The Gentle Singularity'&lt;/a&gt;&lt;/span&gt; leading to a positive sum game.&lt;/p&gt;
&lt;p&gt;Powering every AI output is an intricate mosaic of human knowledge – knowledge created by countless professionals, creators, and experts. By leveraging this vast mosaic, AI can complete routine tasks such as writing reports, summarizing corpora, and drafting presentations in seconds. But the AI that powers these tasks must be told what to do and are then judged on how they did. The "ghosts in the machine" are not alive and they lack human values such as empathy, fairness, dignity, and autonomy. The AI excels at the middle work. Being fluent with AI will mean knowing how to interact with these tools by providing the inputs and knowing how to interpret and use the output.&lt;/p&gt;
&lt;p&gt;This idea runs through the PwC 2025 Global AI Jobs Barometer, which analyzed nearly a billion job postings. They found that industries using AI report three times higher productivity growth and that professionals with AI skills commanded a 56% wage premium over those without AI skills. The message is clear: AI adoption is less about job displacement, and more about AI augmentation where workers shift from routine tasks to strategy, oversight, and problem solving. Human+AI teams outperform either individually.&lt;/p&gt;
&lt;h2 id="the-higher-education-crossroads"&gt;The Higher Education Crossroads&lt;/h2&gt;
&lt;p&gt;The challenge for higher education is how do we meet this new and fast-growing market requirement, especially when constrained by the traditional academic bureaucracy? As a simple example, how can we justify teaching marketing students the same way in a world where Meta plans to automate&lt;label for="mn-meta" class="margin-toggle"&gt;⊕&lt;/label&gt;&lt;input type="checkbox" id="mn-meta" class="margin-toggle"/&gt;&lt;span class="marginnote"&gt;&lt;a href="https://www.theverge.com/meta/659506/mark-zuckerberg-ai-facebook-ads"&gt;Meta's AI advertising plans&lt;/a&gt;&lt;/span&gt; its online advertising business? Short-sighted thinking places us on the path to our own obsolescence. The alternative view is focusing on the needs of the student. A recent study showed that AI tends to complement human skills&lt;label for="mn-study" class="margin-toggle"&gt;⊕&lt;/label&gt;&lt;input type="checkbox" id="mn-study" class="margin-toggle"/&gt;&lt;span class="marginnote"&gt;&lt;a href="https://arxiv.org/abs/2412.19754"&gt;Research on AI and human skills&lt;/a&gt;&lt;/span&gt; like digital literacy, teamwork and resilience far more than it could substitute for them, and that employees who embraced this collaboration experienced wage gains.&lt;/p&gt;
&lt;p&gt;Student-centric thinking highlights that graduates need more than domain specific know-how; they require AI fluency and experience in applying AI in their domain. Meeting this requirement means approaching our educational mission with a different mindset. First, we need to embrace AI and AI tools in the educational process. This ensures students get the needed experience, which provides a new and strong argument for increased experiential learning. Second, we need to consider curricula revisions to prepare students for this AI augmentation of existing careers and new careers that arise from increased adoption of AI.&lt;/p&gt;
&lt;p&gt;This means reimagining business education. Rather than a focus on the mastery of tasks, we must focus on the partnership between human and AI that can lead to better outcomes for all. By confronting this challenge, we can transform the ghosts in the machine from harbingers of doom to partners in a brighter future.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Originally published by the &lt;a href="https://giesbusiness.illinois.edu/news/2025/08/06/ghosts-in-the-machine--the-human-legacy-powering-our-digital-future"&gt;Gies College of Business&lt;/a&gt; on August 6, 2025.&lt;/p&gt;</content><category term="Thoughts"/></entry></feed>